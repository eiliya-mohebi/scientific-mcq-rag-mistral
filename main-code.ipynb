{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "challenge_intro",
      "metadata": {
        "id": "challenge_intro"
      },
      "source": [
        "# In The Name Of GOD\n",
        "# Turing Apex Challenge Submission\n",
        "\n",
        "## Challenge Overview\n",
        "\n",
        "**Scenario:** The year is 2050, and AI models now compete in the Turing Apex Challenge—a global tournament where only the most advanced AI survives. The world's top research teams train their models to tackle ultra-complex scientific problems with minimal human intervention.\n",
        "\n",
        "**Task:** You will be given a training dataset consisting of 50 high-stakes scientific questions, each presented as a multiple-choice question with 4 options, along with their correct answers. Your goal is to develop and optimize an AI model capable of answering similar scientific questions accurately.\n",
        "\n",
        "\n",
        "**Evaluation:** Your model’s performance will be judged solely on Accuracy on the unseen test questions.\n",
        "\n",
        "**Core Requirements:**\n",
        "1. Use an open-source LLM.\n",
        "2. Implement RAG using external data.\n",
        "3. Solution must be a runnable Google Colab notebook.\n",
        "\n",
        "## This Notebook's Approach\n",
        "\n",
        "1.  **LLM:** `mistralai/Mistral-7B-Instruct-v0.3` (Quantized for Colab)\n",
        "2.  **RAG:** Wikipedia Retriever with LLM-based Query Expansion.\n",
        "3.  **Technique:** Few-Shot Prompting + Chain-of-Thought (CoT)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown_installs",
      "metadata": {
        "id": "markdown_installs"
      },
      "source": [
        "## 1. Setup: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "installs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "installs",
        "outputId": "67d2dba2-68f9-4148-a913-14569114d571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU transformers accelerate bitsandbytes\n",
        "!pip install -qU langchain langchain_community langchain_core\n",
        "!pip install -qU wikipedia pandas scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown_imports",
      "metadata": {
        "id": "markdown_imports"
      },
      "source": [
        "## 2. Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports_config",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imports_config",
        "outputId": "92c26e56-42ac-49c6-9c34-794c6e0833ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logging into Hugging Face Hub...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from huggingface_hub import login\n",
        "\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_community.retrievers import WikipediaRetriever\n",
        "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel\n",
        "\n",
        "# Put your HF_Token for connecting to hugging face also you can set it in colab!\n",
        "hf_token = \"HF_TOKEN\"\n",
        "if hf_token:\n",
        "    print(\"Logging into Hugging Face Hub...\")\n",
        "    login(token=hf_token)\n",
        "else:\n",
        "    print(\"HF_TOKEN secret not found!!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown_load_llm",
      "metadata": {
        "id": "markdown_load_llm"
      },
      "source": [
        "## 3. Load Open-Source LLM (Mistral-7B-Instruct)\n",
        "\n",
        "Loads the `mistralai/Mistral-7B-Instruct-v0.3` model using 4-bit quantization for memory efficiency on Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load_llm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634,
          "referenced_widgets": [
            "fdd0f0a247944506958f0a16e9c77073",
            "8f9ab8b3bacb4191927b1e8f6a145d87",
            "74b23fa7023148a39cdb4d49064ddb48",
            "a3e49be84a764cd599fe5b7d1e7f7829",
            "80a2de37673a4cf3ac775f0407436b4c",
            "243b83422b884887b134d82482e4630f",
            "ec00e628bce04ac3bae7436b07aad3a4",
            "95ef80c5882c48b78b8a747131ac6148",
            "80dbd2fcf93f4f9ea5fc1c81e57266a1",
            "66659e6eb34c483ab89f90e7052c7eb8",
            "585785fac0ee41ae9f0ccc76d149535c",
            "ac0747a5ba064c8ab004b843e70380d1",
            "7119520635a64373ad04563303858e6a",
            "7e2cc87b3be441b28654037d67d632e3",
            "dde38894ebeb4b65ab9d24423d7b82e5",
            "3300df0ab42b4adc8fcdc202769778cf",
            "d3e50f59096e4e9996a07e0b6650e99c",
            "85f731c0cbdc49a0b91e8b36591026cb",
            "b4826ff35063475d8d8e568cc5a8990d",
            "e911563dd0544a7991f2d1d8e629089c",
            "1ca239a60c5840f299b0526ffe4ecf7f",
            "c479588ab991430b8d8e2ac8d3d321c7",
            "4039c51fab31402cbae1247ef91bafff",
            "2832e915331e4fadbfb7b34f56e64656",
            "d5f80091fdcc4c0292c282920204e287",
            "50f46f0356bd4dc7b5d6a724710f63fb",
            "edbb80020475465cad61fa5231461cd9",
            "ee1143c357b24af2a847fe3f14206c7b",
            "762a2a0bd69443618df4e366b0d2daf7",
            "430a5b91100548639bb3cc02d0b28100",
            "a5909c2420e9403c8a962f606ea49a7f",
            "f7954f11b78349db8ff7480747acc9b0",
            "58692a3edc474ae2b14ebc9ee3c2eb1d",
            "4b17c3bd613a446f8b5ca1e555027587",
            "0a17d119ab934473a79699126d7d7166",
            "bf9b594b2a484252917ff78f562d2b0d",
            "4de336220662475bac69440e6c0777b1",
            "e4ca8890ad054d0d9c871720e533b34a",
            "dafaab62a0724257bbb866f6a5d18be1",
            "b88d6289bd80400080fa1f1a1c013bc9",
            "a3a8d66e6091498d90bdffdda37b9952",
            "bec0562f4f6f43f9aae098a947181d80",
            "93344e4ba55c4716bf43066a5bba152b",
            "20d7c93a58234b7e8190e1c265873055",
            "a203c4f3ca7c42c6986c6f11daafd5e6",
            "d06a3b33580349aa80e025b42560c385",
            "6581395b63e3481ba7aafd50273bb90e",
            "04803f0f648c432684b68047e9afb7da",
            "ea413d3d8bc24681a4431f486c239844",
            "80725e651b794a499b57f8a8f826c8c9",
            "97cd8e18bbaf4b838afd72c7139c7012",
            "b62a44acc7454049a19d182a68092b07",
            "3cb44dd140754384a1e27c2e4a751765",
            "881acdce706f4615b673945d2f36d504",
            "2f0e43afc846440983f9d16761ecbcd2",
            "9b3fa896d54743ebad9519d31e795a39",
            "62699e324edc4545b900a7b097a17396",
            "5850b7fe879c44fb9cf2f7537138866a",
            "569532b383a440a990b714b0d75d81e0",
            "89c06c55579d448bacf993d39a42f465",
            "7498e8dee37b4176b6be55e30e688f2b",
            "47914c3e2bdf4e9bad45e8a60ad0156b",
            "c924a59a5c9944f89db658fcb13bf962",
            "6c43840214cb40bd8ee954c23000e15a",
            "48710916ea134d1da37d437c659eab43",
            "e3eddd3e4e5c4ef3985e1e1b087e449d",
            "3539860845fa4743a584edec67f8df2a",
            "41cda1dafedc44f287783fcfb4c455aa",
            "9a04a25959054789bbc9ee7ea3574bc2",
            "27ce59ca482a41638e2ca0cac32ff5ba",
            "68f7d5b357714841ae699e02b774d1fc",
            "e5ddf04775ff4124825c19a815b62684",
            "b9aac99b744c411bb7bb9bf199aeaedd",
            "9bf66ab41429426bacbe5ec7d7710c93",
            "675e0b9896184eee9898c64e469825de",
            "026a26c7c8b2415fa1aea4571d1c2684",
            "5cfe3178b9664cb8b72ae19b631861b0",
            "0addcd79d2194ef384ee3bde2a522cc7",
            "40b93a01e4874e228bc21c517e8d51a7",
            "dd74f21bb32744458150244897a540b7",
            "5363042cc4d946d382314df19320f964",
            "ce9fdb0aa6f6460fbbc68c595c4eb868",
            "99e75339e51e4a83aa551c517f2ff892",
            "4ad946762df743cbb924439f4f92d80b",
            "ab7e4551bf574a10918d5b8ddb994bdf",
            "85a54f73c26d4f31aa0a6e175974e150",
            "4a571f81441841399808a3989b4fac72",
            "ec8cfbae3dbc47b9a9527effabd44720",
            "32cacdf156504db3a482cc5342e4bbcf",
            "83d3e4579ab54d23a89a0385cba8f82e",
            "dfda829af02945abbc21b4008df94ade",
            "7f070718cb0445dd8a7b5bae0a3bba60",
            "c24d7c0c7b38434c800387a73fab0952",
            "c96dae76bbb7408d89959306fbdf9c9e",
            "36e3f578053d483889399117e436ec1a",
            "7f55a757e4c54bb2b20e66100c36d6e6",
            "a87ac86e8f8047899d556fa9d1538907",
            "93ff40aba57143bf8fc140a997f5b027",
            "02f53aa6abdb46b49b955a3b0ac0d887",
            "b1633a1d678749a5b4ce40ad001a78cc",
            "5443a900034d48e6a0e18999ae1cab58",
            "152edb5a80884f56ac40370377f355f5",
            "d5293261ae4441c7b7b3c30af92d2228",
            "1bb2eed7b9dc4bcbb337cac0cf6c9d22",
            "fae932f86e984d5d9234d68e47d6a373",
            "0045db40bc60472894b839a330b54a45",
            "394145f675264a7ea7a7cb8fc81eb830",
            "4750895ec1d646e48658ee1dc69a6ddb",
            "bcc95788e8104c6c9cc06ba2e6047b77",
            "be28528799df4872a4d37b1b78bf6788",
            "af84274beb7d42369f88c1fe5ef3bce3",
            "6b79320db7a34557839000a15c922ac4",
            "0c39b15a2183450e8e2b41f8e3a60f80",
            "dd4c5b0212094c86bf61d34a83fd354a",
            "0bd5f2be5ad54029872a3abd4c281b9a",
            "e4df1111b521401e9a19cf4149867bfd",
            "7e9742c253e140a99b1ff13ab5985e02",
            "3864e4121c44460eb0b97201557b1cbf",
            "011398f08b5e44e4a37e286eb720b9ca",
            "7648e3d7f0714756bd4fcc4f08d16b4d",
            "d6c1825253474373adebdd3275473eb4",
            "1e750306d7f3451fb88aa291d5508ffc",
            "6a65b40b3e52496996fb4f1fb419f099",
            "6be93074db5a400687d65fa842774f51",
            "0137a6bf85ab4070991ed489cd859748",
            "45dd890e001d45d5a58070d54905733d",
            "1b9ba5934a1240028ae14980924d9642",
            "0db97e51ea48458e85e0ecc3553ce7bb",
            "eb7dff6bef884e0aa295751638528cf4",
            "469d6fcc2639461fb7b5b98591a91a05",
            "f355f8dab5f94c9b908b9724be0b06a4",
            "29e1130282644487aede4b03992f2cee"
          ]
        },
        "id": "load_llm",
        "outputId": "ea6c4922-0c49-481e-bf7b-715eb373a963"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer for mistralai/Mistral-7B-Instruct-v0.3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdd0f0a247944506958f0a16e9c77073",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac0747a5ba064c8ab004b843e70380d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4039c51fab31402cbae1247ef91bafff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b17c3bd613a446f8b5ca1e555027587",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model mistralai/Mistral-7B-Instruct-v0.3...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a203c4f3ca7c42c6986c6f11daafd5e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b3fa896d54743ebad9519d31e795a39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3539860845fa4743a584edec67f8df2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0addcd79d2194ef384ee3bde2a522cc7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32cacdf156504db3a482cc5342e4bbcf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1633a1d678749a5b4ce40ad001a78cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af84274beb7d42369f88c1fe5ef3bce3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e750306d7f3451fb88aa291d5508ffc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-14a332211a76>:38: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n"
          ]
        }
      ],
      "source": [
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Loading tokenizer for {model_id}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "print(f\"Loading model {model_id}...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=300, # Sufficient length for CoT reasoning + answer\n",
        "    temperature=0.1,   # Low temperature for more deterministic output\n",
        "    do_sample=True     # Required for temperature to work!!!\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown_load_data",
      "metadata": {
        "id": "markdown_load_data"
      },
      "source": [
        "## 4. Load Data\n",
        "\n",
        "Loads the training data (`train_data.csv`) required for few-shot examples and the test data (`test_data.csv`) for generating final predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load_data",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "load_data",
        "outputId": "ee4ef3e8-779a-4572-f7a4-1432d5129997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 50 training examples from 'train_data.csv'.\n",
            "Loaded 150 test examples from 'test_data.csv'.\n"
          ]
        }
      ],
      "source": [
        "train_file = \"train_data.csv\"\n",
        "test_file = \"test_data.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_file)\n",
        "test_df = pd.read_csv(test_file)\n",
        "print(f\"Loaded {len(train_df)} training examples from '{train_file}'.\")\n",
        "print(f\"Loaded {len(test_df)} test examples from '{test_file}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown_retriever",
      "metadata": {
        "id": "markdown_retriever"
      },
      "source": [
        "## 5. Setup RAG Retriever (Wikipedia)\n",
        "\n",
        "Configures the base `WikipediaRetriever` from LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup_retriever",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup_retriever",
        "outputId": "fe59d5a4-ab3e-404f-803e-a2fc15da33da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wikipedia retriever configured.\n"
          ]
        }
      ],
      "source": [
        "# Configure the retriever to fetch 2 documents per query\n",
        "wiki_retriever = WikipediaRetriever(\n",
        "    lang=\"en\",\n",
        "    load_max_docs=2,\n",
        "    top_k_results=2,\n",
        "    doc_content_chars_max=2000 # Max characters per document\n",
        "    )\n",
        "print(\"Wikipedia retriever configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown_query_expansion",
      "metadata": {
        "id": "markdown_query_expansion"
      },
      "source": [
        "## 6. Setup Query Expansion\n",
        "\n",
        "Defines the logic to use the LLM for generating multiple search queries based on the original question, aiming to improve the relevance of retrieved context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup_query_expansion",
      "metadata": {
        "id": "setup_query_expansion"
      },
      "outputs": [],
      "source": [
        "# Prompt template asking the LLM to generate search queries\n",
        "query_expansion_template = \"\"\"You are an AI assistant helping retrieve relevant information.\n",
        "Based on the following user question, generate 3 diverse and effective search queries suitable for finding accurate information on Wikipedia.\n",
        "Output *only* the queries, each on a new line. Do not include numbering or bullet points.\n",
        "\n",
        "User Question: {original_question}\n",
        "\n",
        "Search Queries:\n",
        "\"\"\"\n",
        "query_expansion_prompt = PromptTemplate(\n",
        "    input_variables=[\"original_question\"],\n",
        "    template=query_expansion_template\n",
        ")\n",
        "\n",
        "query_expansion_chain = query_expansion_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Function to parse the LLM query output into a list\n",
        "def parse_expanded_queries(query_string: str) -> list[str]:\n",
        "    queries = query_string.strip().split('\\n')\n",
        "    return [q.strip() for q in queries if q.strip()]\n",
        "\n",
        "# Function to generate and parse expanded queries for a question\n",
        "def get_expanded_queries(original_question: str) -> list[str]:\n",
        "    expanded_query_output = query_expansion_chain.invoke({\"original_question\": original_question})\n",
        "    return parse_expanded_queries(expanded_query_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown_few_shot",
      "metadata": {
        "id": "markdown_few_shot"
      },
      "source": [
        "## 7. Prepare Few-Shot Examples\n",
        "\n",
        "Selects a few examples from the training data to provide in-context learning for the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prepare_few_shot",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prepare_few_shot",
        "outputId": "98417d20-7cbd-4d2b-ddb8-b97394d6fa11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared 5 few-shot examples.\n"
          ]
        }
      ],
      "source": [
        "few_shot_examples = []\n",
        "num_few_shot = 5\n",
        "\n",
        "example_indices = list(np.random.randint(low =0,high=50,size=(num_few_shot,)))\n",
        "for i in example_indices:\n",
        "    row = train_df.loc[i]\n",
        "    example = {\n",
        "        \"question\": row['prompt'],\n",
        "        \"A\": row['A'], \"B\": row['B'], \"C\": row['C'], \"D\": row['D'], \"E\": row['E'],\n",
        "        \"answer\": row['answer']\n",
        "    }\n",
        "    few_shot_examples.append(example)\n",
        "print(f\"Prepared {len(few_shot_examples)} few-shot examples.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown_prompt_template",
      "metadata": {
        "id": "markdown_prompt_template"
      },
      "source": [
        "## 8. Create Prompt Template With Chain-of-Thought (CoT) Technique\n",
        "\n",
        "Defines the main prompt structure using `FewShotPromptTemplate`. It includes instructions, the few-shot examples , the retrieved context, the target question/options, and instructions for CoT reasoning and the final answer format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create_prompt_template",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "create_prompt_template",
        "outputId": "52c8e67f-3e07-4ce0-d671-c383d0953b5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few-shot CoT prompt template created.\n"
          ]
        }
      ],
      "source": [
        "example_prompt_template = \"\"\"\n",
        "Question: {question}\n",
        "A) {A}\n",
        "B) {B}\n",
        "C) {C}\n",
        "D) {D}\n",
        "E) {E}\n",
        "Correct Answer: {answer}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"A\", \"B\", \"C\", \"D\", \"E\", \"answer\"],\n",
        "    template=example_prompt_template\n",
        ")\n",
        "\n",
        "prefix = \"\"\"\n",
        "You are an expert scientific assistant. Answer the following multiple-choice question accurately based on the provided context and examples.\n",
        "Your final response MUST be ONLY one of the uppercase letters A, B, C, D, or E, delivered in the specific format 'LLM Answer: [Letter]'.\n",
        "\n",
        "Here are some examples showing the required output format:\n",
        "\"\"\"\n",
        "\n",
        "suffix = \"\"\"\n",
        "---Relevant Context Start---\n",
        "{context}\n",
        "---Relevant Context End---\n",
        "\n",
        "Now, answer the following question using ONLY the context provided above and your general scientific knowledge:\n",
        "\n",
        "Question: {question}\n",
        "A) {A}\n",
        "B) {B}\n",
        "C) {C}\n",
        "D) {D}\n",
        "E) {E}\n",
        "\n",
        "Think step-by-step to determine the best answer based ONLY on the provided context and question. Explain your reasoning clearly.\n",
        "If the context is incomplete or conflicting, you must still choose the single most likely answer from options A, B, C, D, or E and provide it in the final format. Do NOT output any other text.\n",
        "Provide the final answer *only* in the specified format 'LLM Answer: [Letter]' on a new line immediately after your reasoning.\n",
        "Do not include the reasoning steps on the final answer line itself or any other text.\n",
        "\n",
        "Step-by-step thinking:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Create the FewShotPromptTemplate\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=few_shot_examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"context\", \"question\", \"A\", \"B\", \"C\", \"D\", \"E\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")\n",
        "print(\"Few-shot CoT prompt template created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown_rag_chain",
      "metadata": {
        "id": "markdown_rag_chain"
      },
      "source": [
        "## 9. Build the Enhanced RAG Chain\n",
        "\n",
        "Constructs the final LangChain Expression Language (LCEL) pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "build_rag_chain",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "build_rag_chain",
        "outputId": "f077e205-4260-4cf5-d163-9146fc903199"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG chain with query expansion defined.\n"
          ]
        }
      ],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Function to retrieve context using expanded queries\n",
        "def retrieve_expanded_context(input_dict):\n",
        "    original_question = input_dict['question']\n",
        "    # Generate expanded queries using the previously defined chain\n",
        "    queries = get_expanded_queries(original_question)\n",
        "    # Combine original question with expanded ones for complex search\n",
        "    all_queries = [original_question] + queries\n",
        "\n",
        "    all_retrieved_docs = []\n",
        "    # Retrieve documents for each query\n",
        "    for q in all_queries:\n",
        "        retrieved = wiki_retriever.invoke(q)\n",
        "        all_retrieved_docs.extend(retrieved)\n",
        "\n",
        "    # De-duplicate retrieved documents based on source URL to avoid redundancy\n",
        "    unique_docs = {}\n",
        "    for doc in all_retrieved_docs:\n",
        "        source_url = doc.metadata.get('source', doc.page_content[:100])\n",
        "        if source_url not in unique_docs:\n",
        "            unique_docs[source_url] = doc\n",
        "\n",
        "    # Format the unique documents into a single context string\n",
        "    formatted_context = format_docs(list(unique_docs.values()))\n",
        "    return formatted_context\n",
        "\n",
        "# Define the final RAG chain using LCEL\n",
        "rag_chain = (\n",
        "    # Use RunnableParallel to manage inputs for context retrieval and the main prompt\n",
        "    RunnableParallel(\n",
        "        # Generate context using the expanded retrieval function\n",
        "        context=RunnableLambda(retrieve_expanded_context),\n",
        "        question=RunnablePassthrough(),\n",
        "        A=RunnablePassthrough(),\n",
        "        B=RunnablePassthrough(),\n",
        "        C=RunnablePassthrough(),\n",
        "        D=RunnablePassthrough(),\n",
        "        E=RunnablePassthrough()\n",
        "    )\n",
        "    # Pipe the combined dictionary (with context) into the prompt template\n",
        "    | few_shot_prompt\n",
        "    # Pipe the formatted prompt into the LLM\n",
        "    | llm\n",
        "    # Parse the LLM string output\n",
        "    | StrOutputParser()\n",
        ")\n",
        "print(\"RAG chain with query expansion defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown_parser",
      "metadata": {
        "id": "markdown_parser"
      },
      "source": [
        "## 10. Define Final Output Parser Function\n",
        "\n",
        "Defines the function to extract the single-letter answer (A-E) from the LLM output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "define_parser",
      "metadata": {
        "id": "define_parser"
      },
      "outputs": [],
      "source": [
        "def parse_llm_output(llm_output_text):\n",
        "  pattern = r\"LLM Answer: ([A-E])\"\n",
        "\n",
        "  # Search for the pattern in the input text\n",
        "  match = re.search(pattern, llm_output_text)\n",
        "\n",
        "  if match:\n",
        "    return match.group(1)\n",
        "  else:\n",
        "    # If we can not decode llm output return \"W\" for wrong character!\n",
        "    return \"W\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown_test_predictions",
      "metadata": {
        "id": "markdown_test_predictions"
      },
      "source": [
        "## 11. Generate Predictions on Test Data\n",
        "\n",
        "Iterates through the test dataset runs the complete RAG chain for each question and parse the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1fa5d2e",
      "metadata": {
        "id": "c1fa5d2e",
        "outputId": "a1562635-b8a6-4520-cd07-e9438890a7b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting prediction loop for 150 TEST questions...\n",
            " Question: 1 Parsed: 'D' \n",
            "\n",
            " Question: 2 Parsed: 'W' \n",
            "\n",
            " Question: 3 Parsed: 'B' \n",
            "\n",
            " Question: 4 Parsed: 'B' \n",
            "\n",
            " Question: 5 Parsed: 'C' \n",
            "\n",
            " Question: 6 Parsed: 'B' \n",
            "\n",
            " Question: 7 Parsed: 'W' \n",
            "\n",
            " Question: 8 Parsed: 'D' \n",
            "\n",
            " Question: 9 Parsed: 'B' \n",
            "\n",
            " Question: 10 Parsed: 'W' \n",
            "\n",
            " Question: 11 Parsed: 'A' \n",
            "\n",
            " Question: 12 Parsed: 'D' \n",
            "\n",
            " Question: 13 Parsed: 'E' \n",
            "\n",
            " Question: 14 Parsed: 'C' \n",
            "\n",
            " Question: 15 Parsed: 'B' \n",
            "\n",
            " Question: 16 Parsed: 'A' \n",
            "\n",
            " Question: 17 Parsed: 'B' \n",
            "\n",
            " Question: 18 Parsed: 'B' \n",
            "\n",
            " Question: 19 Parsed: 'B' \n",
            "\n",
            " Question: 20 Parsed: 'E' \n",
            "\n",
            " Question: 21 Parsed: 'E' \n",
            "\n",
            " Question: 22 Parsed: 'C' \n",
            "\n",
            " Question: 23 Parsed: 'W' \n",
            "\n",
            " Question: 24 Parsed: 'C' \n",
            "\n",
            " Question: 25 Parsed: 'D' \n",
            "\n",
            " Question: 26 Parsed: 'B' \n",
            "\n",
            " Question: 27 Parsed: 'A' \n",
            "\n",
            " Question: 28 Parsed: 'A' \n",
            "\n",
            " Question: 29 Parsed: 'D' \n",
            "\n",
            " Question: 30 Parsed: 'B' \n",
            "\n",
            " Question: 31 Parsed: 'B' \n",
            "\n",
            " Question: 32 Parsed: 'B' \n",
            "\n",
            " Question: 33 Parsed: 'B' \n",
            "\n",
            " Question: 34 Parsed: 'A' \n",
            "\n",
            " Question: 35 Parsed: 'C' \n",
            "\n",
            " Question: 36 Parsed: 'D' \n",
            "\n",
            " Question: 37 Parsed: 'E' \n",
            "\n",
            " Question: 38 Parsed: 'E' \n",
            "\n",
            " Question: 39 Parsed: 'W' \n",
            "\n",
            " Question: 40 Parsed: 'B' \n",
            "\n",
            " Question: 41 Parsed: 'C' \n",
            "\n",
            " Question: 42 Parsed: 'E' \n",
            "\n",
            " Question: 43 Parsed: 'B' \n",
            "\n",
            " Question: 44 Parsed: 'D' \n",
            "\n",
            " Question: 45 Parsed: 'A' \n",
            "\n",
            " Question: 46 Parsed: 'D' \n",
            "\n",
            " Question: 47 Parsed: 'A' \n",
            "\n",
            " Question: 48 Parsed: 'W' \n",
            "\n",
            " Question: 49 Parsed: 'D' \n",
            "\n",
            " Question: 50 Parsed: 'A' \n",
            "\n",
            " Question: 51 Parsed: 'D' \n",
            "\n",
            " Question: 52 Parsed: 'E' \n",
            "\n",
            " Question: 53 Parsed: 'W' \n",
            "\n",
            " Question: 54 Parsed: 'B' \n",
            "\n",
            " Question: 55 Parsed: 'D' \n",
            "\n",
            " Question: 56 Parsed: 'C' \n",
            "\n",
            " Question: 57 Parsed: 'E' \n",
            "\n",
            " Question: 58 Parsed: 'C' \n",
            "\n",
            " Question: 59 Parsed: 'A' \n",
            "\n",
            " Question: 60 Parsed: 'B' \n",
            "\n",
            " Question: 61 Parsed: 'B' \n",
            "\n",
            " Question: 62 Parsed: 'D' \n",
            "\n",
            " Question: 63 Parsed: 'D' \n",
            "\n",
            " Question: 64 Parsed: 'A' \n",
            "\n",
            " Question: 65 Parsed: 'C' \n",
            "\n",
            " Question: 66 Parsed: 'A' \n",
            "\n",
            " Question: 67 Parsed: 'A' \n",
            "\n",
            " Question: 68 Parsed: 'A' \n",
            "\n",
            " Question: 69 Parsed: 'B' \n",
            "\n",
            " Question: 70 Parsed: 'E' \n",
            "\n",
            " Question: 71 Parsed: 'E' \n",
            "\n",
            " Question: 72 Parsed: 'A' \n",
            "\n",
            " Question: 73 Parsed: 'W' \n",
            "\n",
            " Question: 74 Parsed: 'A' \n",
            "\n",
            " Question: 75 Parsed: 'W' \n",
            "\n",
            " Question: 76 Parsed: 'C' \n",
            "\n",
            " Question: 77 Parsed: 'A' \n",
            "\n",
            " Question: 78 Parsed: 'C' \n",
            "\n",
            " Question: 79 Parsed: 'A' \n",
            "\n",
            " Question: 80 Parsed: 'A' \n",
            "\n",
            " Question: 81 Parsed: 'E' \n",
            "\n",
            " Question: 82 Parsed: 'C' \n",
            "\n",
            " Question: 83 Parsed: 'E' \n",
            "\n",
            " Question: 84 Parsed: 'B' \n",
            "\n",
            " Question: 85 Parsed: 'C' \n",
            "\n",
            " Question: 86 Parsed: 'A' \n",
            "\n",
            " Question: 87 Parsed: 'A' \n",
            "\n",
            " Question: 88 Parsed: 'W' \n",
            "\n",
            " Question: 89 Parsed: 'A' \n",
            "\n",
            " Question: 90 Parsed: 'B' \n",
            "\n",
            " Question: 91 Parsed: 'B' \n",
            "\n",
            " Question: 92 Parsed: 'A' \n",
            "\n",
            " Question: 93 Parsed: 'A' \n",
            "\n",
            " Question: 94 Parsed: 'B' \n",
            "\n",
            " Question: 95 Parsed: 'C' \n",
            "\n",
            " Question: 96 Parsed: 'C' \n",
            "\n",
            " Question: 97 Parsed: 'D' \n",
            "\n",
            " Question: 98 Parsed: 'C' \n",
            "\n",
            " Question: 99 Parsed: 'W' \n",
            "\n",
            " Question: 100 Parsed: 'A' \n",
            "\n",
            " Question: 101 Parsed: 'D' \n",
            "\n",
            " Question: 102 Parsed: 'A' \n",
            "\n",
            " Question: 103 Parsed: 'D' \n",
            "\n",
            " Question: 104 Parsed: 'C' \n",
            "\n",
            " Question: 105 Parsed: 'E' \n",
            "\n",
            " Question: 106 Parsed: 'A' \n",
            "\n",
            " Question: 107 Parsed: 'E' \n",
            "\n",
            " Question: 108 Parsed: 'C' \n",
            "\n",
            " Question: 109 Parsed: 'E' \n",
            "\n",
            " Question: 110 Parsed: 'D' \n",
            "\n",
            " Question: 111 Parsed: 'W' \n",
            "\n",
            " Question: 112 Parsed: 'C' \n",
            "\n",
            " Question: 113 Parsed: 'D' \n",
            "\n",
            " Question: 114 Parsed: 'C' \n",
            "\n",
            " Question: 115 Parsed: 'C' \n",
            "\n",
            " Question: 116 Parsed: 'B' \n",
            "\n",
            " Question: 117 Parsed: 'C' \n",
            "\n",
            " Question: 118 Parsed: 'B' \n",
            "\n",
            " Question: 119 Parsed: 'C' \n",
            "\n",
            " Question: 120 Parsed: 'D' \n",
            "\n",
            " Question: 121 Parsed: 'D' \n",
            "\n",
            " Question: 122 Parsed: 'E' \n",
            "\n",
            " Question: 123 Parsed: 'E' \n",
            "\n",
            " Question: 124 Parsed: 'C' \n",
            "\n",
            " Question: 125 Parsed: 'A' \n",
            "\n",
            " Question: 126 Parsed: 'B' \n",
            "\n",
            " Question: 127 Parsed: 'E' \n",
            "\n",
            " Question: 128 Parsed: 'A' \n",
            "\n",
            " Question: 129 Parsed: 'C' \n",
            "\n",
            " Question: 130 Parsed: 'B' \n",
            "\n",
            " Question: 131 Parsed: 'A' \n",
            "\n",
            " Question: 132 Parsed: 'W' \n",
            "\n",
            " Question: 133 Parsed: 'B' \n",
            "\n",
            " Question: 134 Parsed: 'A' \n",
            "\n",
            " Question: 135 Parsed: 'D' \n",
            "\n",
            " Question: 136 Parsed: 'B' \n",
            "\n",
            " Question: 137 Parsed: 'D' \n",
            "\n",
            " Question: 138 Parsed: 'B' \n",
            "\n",
            " Question: 139 Parsed: 'W' \n",
            "\n",
            " Question: 140 Parsed: 'D' \n",
            "\n",
            " Question: 141 Parsed: 'W' \n",
            "\n",
            " Question: 142 Parsed: 'E' \n",
            "\n",
            " Question: 143 Parsed: 'D' \n",
            "\n",
            " Question: 144 Parsed: 'D' \n",
            "\n",
            " Question: 145 Parsed: 'D' \n",
            "\n",
            " Question: 146 Parsed: 'A' \n",
            "\n",
            " Question: 147 Parsed: 'B' \n",
            "\n",
            " Question: 148 Parsed: 'B' \n",
            "\n",
            " Question: 149 Parsed: 'W' \n",
            "\n",
            " Question: 150 Parsed: 'W' \n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_predictions = []\n",
        "\n",
        "print(f\"Starting prediction loop for {len(test_df)} TEST questions...\")\n",
        "\n",
        "# Iterate over all test dataframe\n",
        "for index, row in test_df.iterrows():\n",
        "    #input dictionary for the RAG chain\n",
        "    chain_input = {\n",
        "        'question': row['prompt'], 'A': row['A'], 'B': row['B'],\n",
        "        'C': row['C'], 'D': row['D'], 'E': row['E']\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Invoke the full RAG chain\n",
        "        raw_llm_output = rag_chain.invoke(chain_input)\n",
        "        # Parse the final letter answer!\n",
        "        final_answer = parse_llm_output(raw_llm_output)\n",
        "        test_predictions.append(final_answer)\n",
        "\n",
        "        print(f\" Question: {index+1} Parsed: '{final_answer}' \\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing test question {index + 1}: {e} \\n\")\n",
        "        test_predictions.append('W')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qk-KdiUYzRML",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk-KdiUYzRML",
        "outputId": "a1c2bb98-b2d9-4838-949c-575db20bb1d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['D', 'W', 'B', 'B', 'C', 'B', 'W', 'D', 'B', 'W', 'A', 'D', 'E', 'C', 'B', 'A', 'B', 'B', 'B', 'E', 'E', 'C', 'W', 'C', 'D', 'B', 'A', 'A', 'D', 'B', 'B', 'B', 'B', 'A', 'C', 'D', 'E', 'E', 'W', 'B', 'C', 'E', 'B', 'D', 'A', 'D', 'A', 'W', 'D', 'A', 'D', 'E', 'W', 'B', 'D', 'C', 'E', 'C', 'A', 'B', 'B', 'D', 'D', 'A', 'C', 'A', 'A', 'A', 'B', 'E', 'E', 'A', 'W', 'A', 'W', 'C', 'A', 'C', 'A', 'A', 'E', 'C', 'E', 'B', 'C', 'A', 'A', 'W', 'A', 'B', 'B', 'A', 'A', 'B', 'C', 'C', 'D', 'C', 'W', 'A', 'D', 'A', 'D', 'C', 'E', 'A', 'E', 'C', 'E', 'D', 'W', 'C', 'D', 'C', 'C', 'B', 'C', 'B', 'C', 'D', 'D', 'E', 'E', 'C', 'A', 'B', 'E', 'A', 'C', 'B', 'A', 'W', 'B', 'A', 'D', 'B', 'D', 'B', 'W', 'D', 'W', 'E', 'D', 'D', 'D', 'A', 'B', 'B', 'W', 'W']\n"
          ]
        }
      ],
      "source": [
        "print(test_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef938c9a",
      "metadata": {
        "id": "ef938c9a"
      },
      "source": [
        "## Developed By Eiliya Mohebi For DataCoLab Challenge"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env_ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
